{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer -1 Information Gain (IG) measures how much “information” a feature gives us about the target variable. It is used to decide which feature should be chosen to split the dataset at each node of a Decision Tree.\n",
        "\n",
        "Mathematically:\n",
        "IG(S, A) = Entropy(S) - Σ (|Sv|/|S|) × Entropy(Sv)\n",
        "\n",
        "The feature with the highest Information Gain is chosen as the root or split node, since it provides the maximum reduction in uncertainty.\n"
      ],
      "metadata": {
        "id": "tWnvEAetfQ66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer 2 - Entropy measures information content while Gini measures the probability of misclassification.\n",
        "\n",
        "Entropy = -Σ p_i log2(p_i)\n",
        "Gini = 1 - Σ p_i²\n",
        "\n",
        "Entropy is more informative when class probabilities are evenly distributed, while Gini is computationally faster and used in the CART algorithm.\n"
      ],
      "metadata": {
        "id": "PZ7WuWbPfdp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "Answer 3 - Pre-pruning (early stopping) prevents a Decision Tree from growing too deep and overfitting. It stops the tree before it perfectly fits the training data.\n",
        "\n",
        "Methods include setting max depth, minimum samples per split/leaf, and minimum information gain thresholds.\n"
      ],
      "metadata": {
        "id": "WRxpKz6ifj37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Write a Python program to train a Decision Tree Classifier using GiniImpurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Load dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train Decision Tree with Gini Impurity\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=0)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Display feature importances\n",
        "feature_importances = pd.Series(clf.feature_importances_, index=data.feature_names)\n",
        "print(\"Feature Importances:\\n\", feature_importances)"
      ],
      "metadata": {
        "id": "pGnG2T3DfuYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b0eaa4c-ba65-4c0e-d48d-316ac08041c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            " sepal length (cm)    0.000000\n",
            "sepal width (cm)     0.013333\n",
            "petal length (cm)    0.064056\n",
            "petal width (cm)     0.922611\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer 5 - Support Vector Machine (SVM) is a supervised learning algorithm that finds the optimal hyperplane separating classes with the maximum margin. Support vectors are the points closest to the hyperplane\n"
      ],
      "metadata": {
        "id": "8Z2uPQe2f5c8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer 6 - The Kernel Trick allows SVMs to classify non-linearly separable data by mapping it to a higher-dimensional space.\n",
        "\n",
        "Common Kernels:\n",
        "Linear: K(x,y)=x·y\n",
        "Polynomial: K(x,y)=(x·y + c)^d\n",
        "RBF: K(x,y)=exp(-γ||x−y||²)\n",
        "\n",
        "It computes inner products in high-dimensional space without explicit transformation.\n"
      ],
      "metadata": {
        "id": "IQgKB8rvhnvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Python Program – SVM with Linear and RBF Kernels\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print('Linear Kernel Accuracy:', acc_linear)\n",
        "print('RBF Kernel Accuracy:', acc_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2oDlYoihs-a",
        "outputId": "e32dfaa1-858d-47a7-84ad-54ccad3dc7c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 1.0\n",
            "RBF Kernel Accuracy: 0.8055555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called 'Naïve'?\n",
        "\n",
        "Answer 8 - Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem:\n",
        "P(C|X) = P(X|C)*P(C)/P(X)\n",
        "\n",
        "It assumes all features are independent given the class label — this unrealistic assumption makes it 'naïve'. Despite this, it performs well in text classification and spam detection\n"
      ],
      "metadata": {
        "id": "-M_W33AghswK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes\n",
        "\n",
        "Answer 9 - Gaussian NB: For continuous data, assumes normal distribution.\n",
        "Multinomial NB: For count data, like word frequencies.\n",
        "Bernoulli NB: For binary data, like presence/absence of a feature.\n",
        "\n",
        "Examples:\n",
        "- Gaussian: Iris dataset\n",
        "- Multinomial: Text classification\n",
        "- Bernoulli: Spam filtering\n"
      ],
      "metadata": {
        "id": "St1hcvNphr_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Python Program – Gaussian Naïve Bayes on Breast Cancer Dataset\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print('Gaussian Naive Bayes Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "id": "GTEFOJl5iAAV",
        "outputId": "8f3895d6-5cb0-423e-c818-2c970d9373e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    }
  ]
}